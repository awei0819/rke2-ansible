---
# RKE2集群部署后配置Playbook
- name: 加载集群配置到全局变量
  hosts: localhost
  connection: local
  gather_facts: no
  vars_files:
    - ../cluster.yaml
  tasks:
    - name: 设置全局变量
      set_fact:
        global_local_address: "{{ local_address }}"
        global_data_dir: "{{ data_dir }}"
        global_cni: "{{ cni | default('calico') }}"
        global_calico_net: "{{ calico_net | default('ens*') }}"
        global_master_ingress: "{{ master_ingress | default('true') }}"
        global_worker_ingress: "{{ worker_ingress | default('false') }}"
        global_registries: "{{ registries }}"
      run_once: true

- name: 执行集群配置（仅在local_address节点执行）
  hosts: rke2-masters
  gather_facts: yes
  vars:
    operation: "{{ operation | default('init') }}"
  
  pre_tasks:
    - name: 检查当前节点是否是 local_address
      set_fact:
        is_local_address: "{{ ansible_facts['default_ipv4']['address'] == hostvars['localhost']['global_local_address'] or inventory_hostname == hostvars['localhost']['global_local_address'] }}"
    
    - name: 如果不是local_address则跳过
      meta: end_host
      when: not is_local_address
    
    - name: 从全局变量设置本地变量
      set_fact:
        local_address: "{{ hostvars['localhost']['global_local_address'] }}"
        data_dir: "{{ hostvars['localhost']['global_data_dir'] }}"
        cni: "{{ hostvars['localhost']['global_cni'] }}"
        calico_net: "{{ hostvars['localhost']['global_calico_net'] }}"
        master_ingress: "{{ hostvars['localhost']['global_master_ingress'] }}"
        worker_ingress: "{{ hostvars['localhost']['global_worker_ingress'] }}"
        registries: "{{ hostvars['localhost']['global_registries'] }}"

  tasks:
    - name: 验证当前节点是否有kubectl访问权限
      shell: |
        # 检查kubectl是否可用
        if command -v kubectl >/dev/null 2>&1; then
          echo "kubectl found: $(which kubectl)"
          kubectl version --client 2>/dev/null || echo "kubectl not functional"
        else
          # 检查是否有kubeconfig文件
          if [ -f /root/.kube/config ] || [ -f {{ data_dir }}/agent/kubeconfig.yaml ] || [ -f /etc/rancher/rke2/rke2.yaml ]; then
            echo "kubeconfig found but kubectl not in PATH"
          else
            echo "ERROR: No kubectl or kubeconfig found"
            exit 1
          fi
        fi
      register: kubectl_check
      failed_when: "'ERROR' in kubectl_check.stdout"
      changed_when: false

    - name: 设置环境变量
      shell: |
        # 设置PATH包含rke2目录
        export PATH=$PATH:{{ data_dir }}/bin
        # 设置KUBECONFIG
        if [ -f /root/.kube/config ]; then
          export KUBECONFIG=/root/.kube/config
        elif [ -f {{ data_dir }}/agent/kubeconfig.yaml ]; then
          export KUBECONFIG={{ data_dir }}/agent/kubeconfig.yaml
        elif [ -f /etc/rancher/rke2/rke2.yaml ]; then
          export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
        fi
        echo "PATH set to: $PATH"
        echo "KUBECONFIG set to: $KUBECONFIG"
      register: env_setup
      changed_when: false
    
    - name: 验证kubectl连接
      shell: |
        source /etc/profile 2>/dev/null || true
        export PATH=$PATH:{{ data_dir }}/bin
        if [ -f /root/.kube/config ]; then
          export KUBECONFIG=/root/.kube/config
        elif [ -f {{ data_dir }}/agent/kubeconfig.yaml ]; then
          export KUBECONFIG={{ data_dir }}/agent/kubeconfig.yaml
        elif [ -f /etc/rancher/rke2/rke2.yaml ]; then
          export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
        fi
        kubectl cluster-info 2>/dev/null || kubectl get nodes 2>/dev/null || echo "Checking kubectl access..."
      register: kubectl_test
      changed_when: false
      ignore_errors: true
    
    - name: 显示kubectl连接状态
      debug:
        msg: |
          kubectl连接测试:
          {{ kubectl_test.stdout | default('No output') }}
          Exit code: {{ kubectl_test.rc }}


    # ========== 修改calico配置（只在init时执行） ==========
    - name: 修改calico配置
      block:
        - name: 等待calico资源就绪
          shell: |
            source /etc/profile 2>/dev/null || true
            export PATH=$PATH:{{ data_dir }}/bin
            if [ -f /root/.kube/config ]; then
              export KUBECONFIG=/root/.kube/config
            elif [ -f {{ data_dir }}/agent/kubeconfig.yaml ]; then
              export KUBECONFIG={{ data_dir }}/agent/kubeconfig.yaml
            elif [ -f /etc/rancher/rke2/rke2.yaml ]; then
              export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
            fi
            
            echo "等待calico installation资源..."
            for i in {1..90}; do
              if kubectl get installation default -n calico-system &> /dev/null; then
                echo "calico installation found!"
                exit 0
              fi
              echo "Attempt $i/90: calico installation not found yet, waiting..."
              sleep 2
            done
            echo "Timeout waiting for calico installation"
            exit 1
          when: 
            - operation == "init"
            - cni == "calico"
            - calico_net | length > 0
        
        - name: 修改calico网卡配置
          shell: |
            source /etc/profile 2>/dev/null || true
            export PATH=$PATH:{{ data_dir }}/bin
            if [ -f /root/.kube/config ]; then
              export KUBECONFIG=/root/.kube/config
            elif [ -f {{ data_dir }}/agent/kubeconfig.yaml ]; then
              export KUBECONFIG={{ data_dir }}/agent/kubeconfig.yaml
            elif [ -f /etc/rancher/rke2/rke2.yaml ]; then
              export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
            fi
            
            echo "修改calico网卡配置为: {{ calico_net }}"
            kubectl patch installation default -n calico-system \
              --type merge \
              -p "{\"spec\":{\"calicoNetwork\":{\"nodeAddressAutodetectionV4\":{\"interface\":\"{{ calico_net }}\", \"firstFound\": null}}}}"
            echo "calico网卡配置已更新"
          when: 
            - operation == "init"
            - cni == "calico"
            - calico_net | length > 0
          register: calico_patch_result
          failed_when: calico_patch_result.rc != 0


    # ========== 给kube-proxy增加label（init和update都要执行） ==========
    - name: 给kube-proxy增加label
      shell: |
        source /etc/profile 2>/dev/null || true
        export PATH=$PATH:{{ data_dir }}/bin
        if [ -f /root/.kube/config ]; then
          export KUBECONFIG=/root/.kube/config
        elif [ -f {{ data_dir }}/agent/kubeconfig.yaml ]; then
          export KUBECONFIG={{ data_dir }}/agent/kubeconfig.yaml
        elif [ -f /etc/rancher/rke2/rke2.yaml ]; then
          export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
        fi
        
        echo "开始给kube-proxy增加label..."
        pods=$(kubectl get pod -n kube-system --show-labels 2>/dev/null | grep kube-proxy | grep -v k8s-app | awk '{print $1}' || echo "")
        if [ -n "$pods" ]; then
          for pod in $pods; do
            echo "Labeling pod: $pod"
            kubectl label pod -n kube-system $pod k8s-app=kube-proxy 2>/dev/null && echo "  Success" || echo "  Failed or already labeled"
          done
        else
          echo "No kube-proxy pods found without k8s-app label"
        fi
      register: label_result
      changed_when: "'Success' in label_result.stdout or 'Labeled' in label_result.stdout"
      when: 
        - operation == "init" or operation == "update"
    
    # ========== 更新ingress配置（只在init时执行） ==========
    - name: 更新ingress配置
      block:
        - name: 等待ingress configmap存在
          shell: |
            source /etc/profile 2>/dev/null || true
            export PATH=$PATH:{{ data_dir }}/bin
            if [ -f /root/.kube/config ]; then
              export KUBECONFIG=/root/.kube/config
            elif [ -f {{ data_dir }}/agent/kubeconfig.yaml ]; then
              export KUBECONFIG={{ data_dir }}/agent/kubeconfig.yaml
            elif [ -f /etc/rancher/rke2/rke2.yaml ]; then
              export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
            fi
            
            echo "等待ingress configmap创建..."
            for i in {1..90}; do
              if kubectl get cm rke2-ingress-nginx-controller -n kube-system &> /dev/null; then
                echo "ingress configmap found!"
                exit 0
              fi
              echo "Attempt $i/90: ingress configmap not found yet, waiting..."
              sleep 2
            done
            echo "Timeout waiting for ingress configmap"
            exit 1
          when: 
            - operation == "init"
            - (master_ingress | bool or worker_ingress | bool)
        
        - name: 更新ingress configmap
          shell: |
            source /etc/profile 2>/dev/null || true
            export PATH=$PATH:{{ data_dir }}/bin
            if [ -f /root/.kube/config ]; then
              export KUBECONFIG=/root/.kube/config
            elif [ -f {{ data_dir }}/agent/kubeconfig.yaml ]; then
              export KUBECONFIG={{ data_dir }}/agent/kubeconfig.yaml
            elif [ -f /etc/rancher/rke2/rke2.yaml ]; then
              export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
            fi
            
            echo "更新ingress configmap..."
            kubectl patch cm rke2-ingress-nginx-controller -n kube-system \
              --type merge \
              -p '{"data": {
                "allow-snippet-annotations": "true",
                "proxy-body-size": "500m",
                "proxy-read-timeout": "1800",
                "proxy-send-timeout": "1800",
                "ssl-redirect": "false"
              }}'
            echo "ingress configmap updated successfully"
          when: 
            - operation == "init"
            - (master_ingress | bool or worker_ingress | bool)
          register: patch_result
          failed_when: patch_result.rc != 0


        - name: 等待ingress pod创建
          shell: |
            source /etc/profile 2>/dev/null || true
            export PATH=$PATH:{{ data_dir }}/bin
            if [ -f /root/.kube/config ]; then
              export KUBECONFIG=/root/.kube/config
            elif [ -f {{ data_dir }}/agent/kubeconfig.yaml ]; then
              export KUBECONFIG={{ data_dir }}/agent/kubeconfig.yaml
            elif [ -f /etc/rancher/rke2/rke2.yaml ]; then
              export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
            fi
            
            echo "等待ingress pod创建 (timeout: 5m)..."
            timeout 300 bash -c '
              while true; do
                # 检查是否有匹配的pod存在（忽略kubectl get的错误输出）
                if kubectl get pods -n kube-system -l app.kubernetes.io/instance=rke2-ingress-nginx,app.kubernetes.io/component=controller | grep -q "ingress-nginx-controller" &>/dev/null ; then
                  echo "Ingress Pod已创建！"
                  break
                fi
                echo "等待Ingress Pod创建中...（已等待 $((SECONDS)) 秒）"
                sleep 5  # 每5秒检查一次
              done'
          when: 
            - operation == "init"
            - (master_ingress | bool or worker_ingress | bool)
          register: restart_result
          failed_when: restart_result.rc != 0
          ignore_errors: true


        - name: 调整ingress daemonset节点调度规则
          shell: |
            source /etc/profile 2>/dev/null || true
            export PATH=$PATH:{{ data_dir }}/bin
            if [ -f /root/.kube/config ]; then
              export KUBECONFIG=/root/.kube/config
            elif [ -f {{ data_dir }}/agent/kubeconfig.yaml ]; then
              export KUBECONFIG={{ data_dir }}/agent/kubeconfig.yaml
            elif [ -f /etc/rancher/rke2/rke2.yaml ]; then
              export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
            fi
            
            # 定义变量（与Ansible变量映射）
            MASTER_INGRESS={{ master_ingress | lower }}
            WORKER_INGRESS={{ worker_ingress | lower }}
            
            echo "当前配置: master_ingress=$MASTER_INGRESS, worker_ingress=$WORKER_INGRESS"
            
            # 场景1：master_ingress=false（不调度到control-plane节点）
            if [ "$MASTER_INGRESS" = "false" ] && [ "$WORKER_INGRESS" = "true" ]; then
              echo "配置Ingress不调度到control-plane节点..."
              kubectl patch ds rke2-ingress-nginx-controller -n kube-system \
                --type merge \
                -p '{
                  "spec": {
                    "template": {
                      "spec": {
                        "nodeSelector": {
                          "kubernetes.io/os": "linux",
                          "node-role.kubernetes.io/control-plane": "!true"
                        }
                      }
                    }
                  }
                }'
            # 场景2：worker_ingress=false（只调度到control-plane节点）
            elif [ "$MASTER_INGRESS" = "true" ] && [ "$WORKER_INGRESS" = "false" ]; then
              echo "配置Ingress仅调度到control-plane节点..."
              kubectl patch ds rke2-ingress-nginx-controller -n kube-system \
                --type merge \
                -p '{
                  "spec": {
                    "template": {
                      "spec": {
                        "nodeSelector": {
                          "kubernetes.io/os": "linux",
                          "node-role.kubernetes.io/worker": "true"
                        }
                      }
                    }
                  }
                }'
            # 场景3：双方同值（不操作）
            else
              echo "master_ingress和worker_ingress同值，不调整调度规则"
              exit 0
            fi
            
            echo "Ingress DaemonSet调度规则调整完成"
          when: 
            - operation == "init"
            - (master_ingress | bool or worker_ingress | bool)
          register: ds_patch_result
          failed_when: ds_patch_result.rc != 0

        - name: 重启ingress pod
          shell: |
            source /etc/profile 2>/dev/null || true
            export PATH=$PATH:{{ data_dir }}/bin
            if [ -f /root/.kube/config ]; then
              export KUBECONFIG=/root/.kube/config
            elif [ -f {{ data_dir }}/agent/kubeconfig.yaml ]; then
              export KUBECONFIG={{ data_dir }}/agent/kubeconfig.yaml
            elif [ -f /etc/rancher/rke2/rke2.yaml ]; then
              export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
            fi
            
            echo "重启ingress pods..."
            kubectl delete pod -n kube-system -l app.kubernetes.io/instance=rke2-ingress-nginx --force --grace-period=0 2>/dev/null || true
            echo "等待ingress pod就绪 (timeout: 5m)..."
            kubectl wait --for=condition=Ready pod -n kube-system -l app.kubernetes.io/instance=rke2-ingress-nginx --timeout=300s
          when: 
            - operation == "init"
            - (master_ingress | bool or worker_ingress | bool)
          register: restart_result
          failed_when: restart_result.rc != 0
          ignore_errors: true
    

    
    # ========== 显示集群状态 ==========
    - name: 显示集群状态
      shell: |
        source /etc/profile 2>/dev/null || true
        export PATH=$PATH:{{ data_dir }}/bin
        if [ -f /root/.kube/config ]; then
          export KUBECONFIG=/root/.kube/config
        elif [ -f {{ data_dir }}/agent/kubeconfig.yaml ]; then
          export KUBECONFIG={{ data_dir }}/agent/kubeconfig.yaml
        elif [ -f /etc/rancher/rke2/rke2.yaml ]; then
          export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
        fi
        
        echo "========================================="
        echo "集群节点状态:"
        kubectl get nodes -o wide 2>/dev/null || echo "无法获取节点状态"
        echo "========================================="
      register: cluster_status
      changed_when: false
    
    - name: 显示最终状态
      debug:
        msg: "{{ cluster_status.stdout_lines }}"